---
title: Using robust features when training an AAM model with in-the-wild data
author: "Yuri Pieters"
date: '`DRAFT - \today`{=latex}'

header-includes:
  - |
    ```{=latex}
    \setkomafont{subject}{ \normalfont\normalcolor\large }
    \setkomafont{publishers}{ \large }

    \titlehead{%
      \hfill Department of Computer Science\\
      \vspace{32pt}
      \begin{center}
        \includegraphics[height=1in]{images/UOY-Logo-Stacked-shield-Black.png}
      \end{center}
    }
    \subject{Submitted in part fulfilment for the degree of BSc.}
    \publishers{Supervisor: Nick Pears}
    ```
  - |
    ```{=latex}
    \setmathrm{TeX Gyre Pagella}
    ```
...

# Executive Summary {.unnumbered}

*the following is an outline*

Aim: The goal of this work was to understand, compare, and evaluate a range of
different techniques used for landmark localisation on faces. In this report I
present the models I chose to evaluate, the datasets I evaluated them against,
and the results of the evaluation. I also present a review of the field overall,
which motivates the particular models and datasets I choose.

Motivation:

- Computer vision is an important part of many topics at the bleeding edge of
  computer science: robotics, self driving cars, human computer interfaces.
- Computer vision techniques aren't one size fits all. To pick between them
  requires understanding of the differences.
- Recognition of faces is an important and frequently used application of
  computer vision.
- Therefore understanding and picking between face recognition algorithms is
  important.

Methods: I evaluated the algorithms by running them on the same set of data, and
calculating how closely they reproduced the true landmark points.

Results: I found that ....

Social and ethical issues of face detection and landmarking:

- Human faces have a huge variety
- Everyone has a right to be able to use the technology that may be supported by
  computer vision
- Therefore both the computer vision techniques and actual implementation of
  those techniques (e.g. the actual training images used) should be developed
  with the full diversity of humans in mind.


# Introduction

This report details a investigation into the use of robust features for Active
Appearance Models, which are a computer vision technique for modeling the shape
and appearance of objects in images.

Computer vision is a field of computer science that deals with getting getting
computers to "see". The subject has a long history, dating back to the 1960s
when it was thought that the problem would be essentially solved over a summer
project [@paper1966a]. It proved significantly more difficult of course, and it
is only with the modern advances in imaging and computing technologies that
progress has accelerated, both by making new things possible and increasing the
demand for solutions to computer vision problems [@forsyPonce2012a]. Fields
where computer vision has been applied include include medical imaging,
robotics, human-computer interaction, security, manufacturing, and more
[@szeli2022a].

<!--

In [@princ2012a, p. 13], Prince formulates the goal of computer vision
succinctly. A paraphrasing of his formulation follows: the vision problem, or
goal, is to use measurements extracted from an image to infer something about
the world state. For example, in object detection the world state may be defined
as the presence of absence of a particular object. The measurements may be the
raw pixel intensity values, or they may be the result of some pre-processing
technique. To achieve this goal, a model is built. The model describes a family
of statistical relationships between the measurements and the world state, and a
particular instance of the family is defined by a set of parameters. In learning
the goal is to choose parameters for the model so that it accurately reflects
the relationship between the features and the world state. In inference we use
the model to tell us something about the world state given a new set of
measurements. The processes for learning and inference are embodied by
algorithms.

-->

Active Appearance Models (AAM) are generative statistical models of appearance
and shape. They parametrically describe the variations in appearance and shape
seen in objects, and can be fit to an unseen image by minimising the difference
between it and the image generated by the model. The resulting model instance is
a relatively compact description of the object that can be used in further
analysis. For example, an instance of an AAM trained on human faces could be
used to estimate head pose from the shape, or gender from the appearance.


## Motivation, aims, and objectives

Recent research has focused on deep learning methods for solving problems of
deformable shape model fitting and object recognition in general. There has been
great success in this area, in many cases pushing the state of the art.
Traditional statistical models have thus fallen by the wayside to some extent.

However these models do have advantages over deep learning methods that should
not be forgotten. Deep learning is still very computationally intensive, for
example, requiring specialised hardware such as GPUs or heavy code optimisation
to use effectively. In many cases traditional techniques are fast enough to run
on ordinary CPUs, while still achieving excellent performance.

Deep learning can also sometimes be treated as a black box, producing good
results that are difficult to explain. Traditional techniques on the other hand
are built on an understanding of every step, allowing a fuller view of how they
do their thing.

This work is in large part inspired by the work in [@tzimiPanti2013a], which
introduced a new AAM fitting algorithm they named Fast-SIC, but has since been
termed alternating inverse-compositional (AIC). They showed that, using this
algorithm to fit an AAM trained on in-the-wild data, state-of-the art
performance could be achieved on generic face fitting problems, even without the
use of robust features.

The aim of this work is to evaluate AAMs that do use robust features against the
current state-of-the-art deep learning methods that require GPUs to use. The
result should illuminate the state of deep learning methods and reveal what is
possible relatively less computationally expensive statistical methods.

To do this, AAMs using several different dense image features will be compared
against off-the-shelf deep learning algorithms in the task of generic face
fitting.

<!--

One of the more popular subjects for computer vision is human faces. Humans have
an intuitive understanding of the human face from a very young age
([@fig:baby]), and we quickly learn to interpret the faces of others to tell us
who they are, where they are looking, what they are feeling, and more; the face
is a rich form of non-verbal communication [@kanwiYovel2009a]. As with other
tasks in computer vision however, trying to infer something as subtle as emotion
from a collection of pixels is a non-trivial task. The solution, of course, is
to pre-process the data somehow to extract only relevant features
[@martiValst2016a]. This breaks a difficult problem (such analysing facial
expressions), into more manageable sub-problems, which can be tackled
separately. One such pre-processing technique that turns out to be useful for
multiple different tasks is facial landmarking [@martiValst2016a;
@murphTrive2009a].

![An expert face analyser[^babyref]](./images/sleeping_baby.jpg){#fig:baby
width=60%}

[^babyref]: "[Sleeping Baby](https://www.flickr.com/photos/biblicone/2533285432/)"
([CC BY-NC-SA 2.0](https://creativecommons.org/licenses/by-nc-sa/2.0/)) by
bikesandwich on Flickr


The goal of facial landmarking is to fit a parameterised shape model to an image
of a face such that its points correspond to consistent locations on the face
[@saragLuceyEtAl2011a]. [@Fig:landmarkExample] shows an example of a face fitted
with such a model. Landmarks points may either correspond to well defined facial
part, such as the tip of the nose or the corner of the eye, or they may be part
of a group marking a boundary, such as the edge of the face. The exact
configuration and meaning of the landmark points can vary between datasets used,
and several different configurations are in use. One of the most popular however
is the 68 point annotation used originally by the Multi-PIE dataset
[@grossMatthEtAl2010a].

```{#fig:landmarkExample .matplotlib dpi=160 tight_bbox=true width=70%
    caption="Example of a face from the 300W face dataset [@sagonAntonEtAl2016a]
    with a set of landmark points annotated."
}
import menpo
img = menpo.io.import_image('Datasets/300w_cropped/01_Indoor/indoor_225.png')
img.landmarks['PTS'] = menpo.landmark.face_ibug_68_to_face_ibug_68(img.landmarks['PTS'])
img.crop_to_landmarks_proportion(0.2).view_landmarks(render_lines=True, marker_size=5)
```

Facial landmarking can be used to as part of the process of solving more
difficult facial analysis problems in different ways. Most obviously, landmarks
can be used directly as input data for a model. A model for head pose estimation
for example may not actually need most of the information encoded in the image
pixels; the pose of the head can be inferred from the relative positions of the
various facial features, which is what the landmarks encode [@murphTrive2009a].
Alternatively, the landmarks can be used as part of additional pre-processing
steps such as registration and feature extraction [@martiValst2016a]. In
registration the idea is to remove variation in rotation and scale; this can be
done by first computing a transformation that places the landmarks onto a
predefined reference shape, and then applying the same transformation to the
image itself. In feature extraction the goal is to compute summaries of the
image data that keeps relevant information while getting rid of nuisance
factors. Landmarks can help localise the features, so that each feature
represents the same part of the face in every example. Features can also be
computed directly on the landmarks themselves, encoding geometric relationships
between different parts of the face.
-->


## Report overview

The structure of the rest of this report is as follows. *to be written once finished*

# Background

## AAM history

AAMs were first explored in the work of Edwards et al. [@edwarTayloEtAl1998a] in
the late 90s. There was much interest at the time in interpretation by
synthesis, a by which images are interpreted by synthesising a parametric
version of the image.

## Advancements in AAMs

The limiting factor for AAMs is largely speed. The appearance model is typically
high dimensional, in the order of 10^1^--10^2^ principle components are typical,
especially with multichannel images (or with multi-channel features as in this
work). Traditional Gauss-Newton optimisation requires computing and inverting
large Hessian matrices to find the gradient and this is computationally
expensive. Advances in AAMs then have mostly focused on this optimisation step.

In the original work on AAMs, computing the parameter update analytically was
prohibitive on the hardware of the time \marginpar{\raggedright Justify this with more
careful reading of the literature}. Therefore they used an additional step
during learning to learn a linear approximation of the update step based on the
image. The idea was to that the gradient direction on the training images would
generalise to unseen images. There several iterations of this idea, improving
the model for the parameter update. However, these methods trade away a fair bit
of accuracy, robustness, and generalisability for their speed.

The other form of improvement for fitting was in the analytical methods for
computing the update, along with increasing computing power making expensive
algorithms more viable. An early breakthrough was the project-out
inverse-compositional (POIC) algorithm of Matthews and Baker [@matthBaker2004a],
which simplified the optimisation problem by decoupling the shape and appearance
variation by "projecting-out" the appearance variation, working a subspace that
is the orthogonal complement of the appearance variation as a result
[@antonAlaboEtAl2015a]. This algorithm is very fast, but not very robust,
sacrificing accuracy for speed. It tends to break down when fitted to an image
with high appearance variation or outliers.

There is also simultaneous inverse composition (SIC), which is a slow but
accurate algorithm. A more recent algorithm, alternating inverse-composition
(AIC), has been shown [@tzimiPanti2013a] to be equivalent to SIC (produces the
same update step) but much faster. While not quite as fast as POIC, AIC is much
more accurate. As AIC is the algorithm used in this work, it is detailed below
in [@sec:inference-algorithm].

Table: Algorithmic complexity for some of the main AAM inference algorithms
[@antonAlaboEtAl2015a]. {#tbl:algorithmicComplexity}

Algorithm Complexity
--------- --------------------
POIC      $\BigO(N_{\symup{S}} L_{\symup{A}} + N_{\symup{S}}^2)$
SIC       $\BigO((N_{\symup{S}} + N_{\symup{A}})^2 L_{\symup{A}} + (N_{\symup{S}} + N_{\symup{A}})^3)$
AIC       $\BigO(N_{\symup{S}}^2 N_{\symup{A}}^2 + (N_{\symup{S}} + N_{\symup{A}})L_{\symup{A}} + N_{\symup{S}}^3)$

In [@tbl:algorithmicComplexity], $N_{\symup{S}}$ is the number of shape
components, $N_{\symup{A}}$ is the number of appearance components, and
$L_{\symup{A}}$ is the length of the appearance vector.

## Image features

## Other landmarking methods

There have been many approaches taken to the problem of landmark fitting, but
they can largely be divided into three categories [@wuJi2019a]: *holistic
methods*, *Constrained Local Models*, and *regression based methods*. The
categories are based on how the facial appearance and facial shape patterns are
modelled and related. For holistic methods, the main example is AAM; the
category is named because the holistic appearance is used to fit the landmarks.
Constrained Local Model (CLM) approaches train a set of independent models for
each of the facial landmarks, but constrain the locations of the landmarks based
on a global model of the face shape. Lastly, the regression based methods do not
explicitly model the global face shape at all, instead directly relating image
data (either local or global) to landmark locations.


### Constrained Local Models

The central idea of CLM is to model, for each landmark, the likelihood that it
should be placed on a certain part of the image, but to then constrain the final
landmark locations to fit a model of the face shape as a whole. 

CLM models can be traced back work by Cristinacce and Cootes
[@cristCoote2006a]. *Describe basic form of CLM*

*Discuss extensions to the algorithms*

### Regression based methods

*Describe class of algorithm and discuss extensions*


# Algorithm details

In this section details are given on the different algorithms used for AAM
fitting in this work.

## AAM design

## Inference algorithm

The inference algorithm used is AIC [@tzimiPanti2013a].

*add details*

## Image features

An image feature is a measurement extracted from an image that attempts to
describe the contents. By this definition, the pixel intensity values themselves
are image features, though typically weak ones. The term feature makes no
guarantees of strength or usefulness. Looked at from another angle, the fitted
AAM instance is itself an image feature. It is an attempt to describe the image
contents after all. Indeed, higher level algorithms may treat AAM as a black box
feature extraction algorithm.

For the purposes of AAM itself however, image features have a few requirements.
There are features such as SURF which summarise the properties of a few
subregions of the image; for AAM this isn't useful as the shape of the image is
lost. The feature must preserve shape. Some features summarise an image in a
regular grid of sub-regions. The default scale-invariant feature transform
(SIFT) and histogram of oriented gradients (HOG) features like this. These
preserve shape, but lose spacial accuracy, as points can only be localised to
the area of the sub-region. Therefore the feature must be dense, i.e. computed
at every pixel.

Features tested were

- Dense HOG
- Dense SIFT
- DAISY
- Image Gradient Orientation (IGO)

*need details*

# Experimental design and implementation

To evaluate the Active Appearance Models which use robust features, some
experiments were carried out. In [@sec:algorithm-implementation] the details are
given on how the evaluated algorithm was implemented; in [@sec:datasets] the
in-the-wild datasets employed for training and testing are detailed; in
[@sec:evaluation-criteria] the methodology for how evaluating performance is
given; finally, [@sec:results] presents the data collected from performing the
experiments.

## Compared algorithms

*List algorithms that AAM is compared against*

## Algorithm implementation

To implement the experiments Python and the `menpo`/`menpofit` packages were
used. The AAMs were trained on the LFPW and IBUG datasets, and test on the 300w
challenge dataset.

The selection of Python was driven by the availability of the `menpo` packages.
`menpo` and `menpofit` are Python packages that implement a framework for
deformable object modelling, including extensible AAM classes.

## Evaluation criteria

The algorithms were evaluated on the point-to-point error between the fitted
shape the ground truth annotations, normalised by the
distance between the outer corners of the eye, as used in
@sagonAntonEtAl2016a. Specifically, given a set of $N$ fitted landmark points
$\vec{s}^{\symup{f}} = \{\vec{x}^{\symup{f}}_i\}_{i=1}^{N}$ and a corresponding set of $N$ ground
truth landmark points $\vec{s}^{\symup{g}} = \{\vec{x}^{\symup{g}}_i\}_{i=1}^{N}$, the error is:

$$
\mathrm{Error} =
\frac{1}{d_{\mathit{outer}}N}\sum_{i=1}^{N}\lVert\vec{x}^{\symup{f}}_i -
\vec{x}^{\symup{g}}_i\rVert
$$

# Results

- What happened
- Analysis of what happened

# Conclusion

- Final wrap up what happened
- Project aims

# Bibliography {.unnumbered}

:::{#refs}
:::
